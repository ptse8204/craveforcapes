{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67295ad2",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e085d79",
   "metadata": {},
   "source": [
    "This notebook will showcase the method to collect the entire dataset using selenium, with a clumsy way to store it initally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ffb91",
   "metadata": {},
   "source": [
    "# Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4929fdd9",
   "metadata": {},
   "source": [
    "## Onboarding and Requirements\n",
    "\n",
    "We will need the following requirements in order to properly scrape the data stored in capes.ucsd.edu. This part of code could not be done without the previous code that have been written by Seascape – https://github.com/dcao/seascape, and Smartercape – https://github.com/andportnoy/smartercapes.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4dd9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc304543",
   "metadata": {},
   "source": [
    "## Functions that would run code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf364a79",
   "metadata": {},
   "source": [
    "In order for the below function to be able to perform correctly, one must install firefox and alongside with webdriver. The function below will return a pandas dataframe that contains all of the general information of the courses that were caped before. If you only need the general info from cape, it is advise that you run this function to a variable and output the pandas dataframe to a csv for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b9f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_cape_dataframe():\n",
    "    \n",
    "    # launch browser using Selenium, need to have Firefox installed\n",
    "    print('Opening a browser window...')\n",
    "    s=Service(GeckoDriverManager().install())\n",
    "    driver = webdriver.Firefox(service=s)\n",
    "    print('Browser window open, loading the page...')\n",
    "\n",
    "    # get the page that lists all the data, first try\n",
    "    driver.get('https://cape.ucsd.edu/responses/Results.aspx')\n",
    "    print('Please enter credentials...')\n",
    "\n",
    "    # wait until SSO credentials are entered\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    element = wait.until(expected_conditions.title_contains('Course And Professor Evaluations (CAPE)'))\n",
    "\n",
    "    # get the page that lists all the data\n",
    "    # (%2C is the comma, drops all the data since every professor name has it)\n",
    "    driver.get('https://cape.ucsd.edu/responses/Results.aspx?Name=%2C')\n",
    "\n",
    "    # read in the dataset from the html file\n",
    "    df = pd.read_html(driver.page_source)[0]\n",
    "    print('Dataset parsed, closing browser window.')\n",
    "\n",
    "    # destroy driver instance\n",
    "    driver.quit()\n",
    "\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bfa319",
   "metadata": {},
   "source": [
    "The function below will return a beautiful soup object, using the same method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "212acd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_html():\n",
    "    # launch browser using Selenium, need to have Firefox installed\n",
    "    print('Opening a browser window...')\n",
    "    s=Service(GeckoDriverManager().install())\n",
    "    driver = webdriver.Firefox(service=s)\n",
    "    print('Browser window open, loading the page...')\n",
    "\n",
    "    # get the page that lists all the data, first try\n",
    "    driver.get('https://cape.ucsd.edu/responses/Results.aspx')\n",
    "    print('Please enter credentials...')\n",
    "\n",
    "    # wait until SSO credentials are entered\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    element = wait.until(expected_conditions.title_contains('Course And Professor Evaluations (CAPE)'))\n",
    "    print('The driver is now loading the page, please wait patiently as it may take longer than\\n' +\n",
    "    'a few minutes to complete due to the large size of the CAPE dataset')\n",
    "\n",
    "    # get the page that lists all the data\n",
    "    # (%2C is the comma, drops all the data since every professor name has it)\n",
    "    driver.get('https://cape.ucsd.edu/responses/Results.aspx?Name=%2C')\n",
    "    print('Finished loading, currently parsing the soup object...')\n",
    "    \n",
    "    # creating soup object\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    print('Dataset parsed, closing browser window.')\n",
    "\n",
    "    # destroy driver instance\n",
    "    driver.quit()\n",
    "\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8578c76",
   "metadata": {},
   "source": [
    "In order to get all the course links to get all the nitty gritty of cape, one must need to get all the links to scrape the page one by one. The function below would get all the link that the general page provides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26189f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_courseID(soup):\n",
    "    bs4obj = soup.find_all(\"a\", {\"href\" : True, \"id\": re.compile(\"ContentPlaceHolder1_gvCAPEs_hlViewReport_.*\")})\n",
    "    return_lst = []\n",
    "    for query in bs4obj:\n",
    "        return_lst.append(query.get(\"href\"))\n",
    "    return return_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210db7c",
   "metadata": {},
   "source": [
    "After getting all the course links, one must request the page one by one to pull the data. This function would take a page and return a bs4obj to grab the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25de28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bs4(page):\n",
    "    driver.get(page)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932c9c5d",
   "metadata": {},
   "source": [
    "This function would take a soup object and return a dictionary contains a dictionary, which it's value is also a dictionary and the value of that dictionary would be a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e128e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_collector(bs4obj):\n",
    "    question_text = bs4obj.find_all(\"span\", {\"id\": re.compile(\"ContentPlaceHolder1_dlQuestionnaire_lblQuestionText_.*\")})\n",
    "    dct_key = []\n",
    "    general_info_dct = dict(zip([\"Course\", \"Instructor\", \"Qtr/Yr\", \"Enrollment\", \"CAPEs Returned\"], \n",
    "                                           [[i.text] for i in bs4obj.find_all(\n",
    "                                               \"span\", {\"id\": re.compile(\"ContentPlaceHolder1_lbl.*\"), \n",
    "                                                        \"style\": \"font-weight:bold;\"})]))\n",
    "    general_info_dct[\"Question\"] = \"General Info\"\n",
    "    return_dct = {\"General Info\": {\"General Info\": general_info_dct}}\n",
    "    counter = 0\n",
    "    for qt in question_text:\n",
    "        if qt.text.split()[0].isupper() and len(qt.text.split()[0]) > 1:\n",
    "            dct_key.append(qt.text)\n",
    "            return_dct[qt.text] = {}\n",
    "            counter += 1\n",
    "        else:\n",
    "            if len(bs4obj.find_all(\"span\", {\"id\": re.compile(\"ContentPlaceHolder1_dlQuestionnaire_rptChoiceTexts_\" \n",
    "                                                                + str(counter)+\"_.*\")})) != 0:\n",
    "                col = dict(zip([i.text for i in bs4obj.find_all(\"span\", {\"id\": re.compile(\"ContentPlaceHolder1_dlQuestionnaire_rptChoiceTexts_\" \n",
    "                                                                    + str(counter)+\"_.*\")})],\n",
    "                                [[i.get_text(separator=\" \").split(\" \")[0]] for i in bs4obj.find_all(\"span\", {\"id\": re.compile(\"ContentPlaceHolder1_dlQuestionnaire_rptChoices_\" \n",
    "                                                                    + str(counter)+\"_.*\")})]))\n",
    "                col_copy = col.copy()\n",
    "                col_copy.update({\"Question\": qt.text})\n",
    "                return_dct[dct_key[-1]].update({qt.text: col_copy})\n",
    "                counter += 1\n",
    "            else:\n",
    "                col = dict(zip(col.keys(), [[i.get_text(separator=\" \").split(\" \")[0]] for i in bs4obj.find_all(\"span\", {\"id\": re.compile(\"ContentPlaceHolder1_dlQuestionnaire_rptChoices_\" \n",
    "                                                                    + str(counter)+\"_.*\")})]))\n",
    "                col_copy = col.copy()\n",
    "                col_copy.update({\"Question\": qt.text})\n",
    "                return_dct[dct_key[-1]].update({qt.text: col_copy})\n",
    "                counter += 1\n",
    "    return return_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbb20a",
   "metadata": {},
   "source": [
    "This function would take the multi-level dictionary into a single dataframe. However, this resulting dataframe would have lots of null values. The sole goal of this function is to create a single dataframe that is easy for storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3f3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_collector_to_pd(bs4obj):\n",
    "    dict_collect = question_collector(bs4obj)\n",
    "    return_dct = {}\n",
    "    for key, value in dict_collect.items():\n",
    "        for in_key, in_value in value.items():\n",
    "            if len(value[in_key]) != 1:\n",
    "                return_dct[in_key] = pd.DataFrame(data = in_value)\n",
    "                return_dct[in_key].insert(0, \"course\", \n",
    "                                          bs4obj.find(\"span\", {\"id\": \"ContentPlaceHolder1_lblCourseDescription\"})\n",
    "                                          .text.split(\"(\")[0].strip())\n",
    "                return_dct[in_key].insert(0, \"term\", \n",
    "                                          bs4obj.find(\"span\", {\"id\": \"ContentPlaceHolder1_lblTermCode\"}).text)\n",
    "    df_return = pd.DataFrame([])\n",
    "    for values in return_dct.values():\n",
    "        df_return = pd.concat([df_return, values])\n",
    "    df_return = df_return.reset_index()\n",
    "    return df_return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25436eea",
   "metadata": {},
   "source": [
    "# The final function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c15cb3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_html_noquit():\n",
    "    # launch browser using Selenium, need to have Firefox installed\n",
    "    print('Opening a browser window...')\n",
    "    s=Service(GeckoDriverManager().install())\n",
    "    driver = webdriver.Firefox(service=s)\n",
    "    print('Browser window open, loading the page...')\n",
    "\n",
    "    # get the page that lists all the data, first try\n",
    "    driver.get('https://cape.ucsd.edu/responses/Results.aspx')\n",
    "    print('Please enter credentials...')\n",
    "\n",
    "    # wait until SSO credentials are entered\n",
    "    wait = WebDriverWait(driver, 60)\n",
    "    element = wait.until(expected_conditions.title_contains('Course And Professor Evaluations (CAPE)'))\n",
    "    print('Now loading...\\nPlease wait patiently as it may take longer than ' +\n",
    "    'a few minutes to complete due to the large size of the CAPE dataset')\n",
    "\n",
    "    # get the page that lists all the data\n",
    "    # (%2C is the comma, drops all the data since every professor name has it)\n",
    "    driver.get('https://cape.ucsd.edu/responses/Results.aspx?Name=%2C')\n",
    "    print('Finished loading, currently parsing the soup object...')\n",
    "    \n",
    "    # creating soup object\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    print('Soup parsed.')\n",
    "\n",
    "    return soup, driver"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b8a5b87",
   "metadata": {},
   "source": [
    "def scrape_func():\n",
    "    soup, driver = get_raw_html_noquit()\n",
    "    print(\"Getting a list of course IDs for scraping the individual pages...\")\n",
    "    course_lst = get_courseID(soup)\n",
    "    print('This browser window will now keep changing until the last cape record has been recorded...')\n",
    "    return_pd = pd.DataFrame([])\n",
    "    for course in course_lst:\n",
    "        course_link = \"https://cape.ucsd.edu/responses/\" + course\n",
    "        # Since we know prior to FA12, CAPE uses old format, we would exclude it from this function\n",
    "        if course_link == \"https://cape.ucsd.edu/scripts/detailedStats.asp?SectionId=749583\":\n",
    "            break\n",
    "        # get each individual course page\n",
    "        driver.get(course_link)\n",
    "        # creating soup object\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        # return an individual dataframe object for one page\n",
    "        return_pd = pd.concat([return_pd, question_collector_to_pd(soup)])\n",
    "    # destroy driver instance\n",
    "    driver.quit()\n",
    "    print(\"Congradualation! The last record has been recorded, now outputing the data to a csv...\")\n",
    "    # outputing the result data to a file named realdata.csv\n",
    "    return_pd.to_csv(\"realdata.csv\")\n",
    "    print('The data has been output to a file named \"realdata.csv\". Goodbye!')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed845c70",
   "metadata": {},
   "source": [
    "def scrape_func():\n",
    "    soup, driver = get_raw_html_noquit()\n",
    "    print(\"Getting a list of course IDs for scraping the individual pages...\")\n",
    "    course_lst = get_courseID(soup)\n",
    "    print('This browser window will now keep changing until the last cape record has been recorded...')\n",
    "    return_pd = pd.DataFrame([])\n",
    "    counter = 0 # counting the slice of csv files ouput\n",
    "    course_link = \"https://cape.ucsd.edu/responses/\" + course_lst[0]\n",
    "    # get each individual course page\n",
    "    driver.get(course_link)\n",
    "    for course in range(len(course_lst)):\n",
    "        try:\n",
    "            # creating soup object\n",
    "            soup = BeautifulSoup(driver.page_source)\n",
    "            # Getting the actual grade received distribution\n",
    "            df = pd.read_html(driver.page_source)\n",
    "            course_link = \"https://cape.ucsd.edu/responses/\" + course_lst[course + 1]\n",
    "            # Since we know prior to FA12, CAPE uses old format, we would exclude it from this function\n",
    "            if course_link == \"https://cape.ucsd.edu/scripts/detailedStats.asp?SectionId=749583\":\n",
    "                break\n",
    "            # get each individual course page\n",
    "            try:\n",
    "                driver.get(course_link)\n",
    "            except:\n",
    "                try:\n",
    "                    driver.get(course_link)\n",
    "                except:\n",
    "                    print(\"Unfortunately, there is an error occured, now outputing the remaining data to a csv...\")\n",
    "                    print(course_link)\n",
    "                    print(course_lst.index(course_link))\n",
    "                    # outputing the result data to a file named realdata.csv\n",
    "                    return_pd.to_csv(\"realdata.csv\")\n",
    "                    print(course_link)\n",
    "\n",
    "            # return an individual dataframe object for one page\n",
    "            return_pd = pd.concat([return_pd, question_collector_to_pd(soup)])\n",
    "            if len(df) > 2:\n",
    "                df = df[2]\n",
    "                df[\"term\"] = return_pd[\"term\"].iloc[-1]\n",
    "                df[\"course\"] = return_pd[\"course\"].iloc[-1]\n",
    "                df[\"Question\"] = \"Grade received\"\n",
    "                return_pd = pd.concat([return_pd, df])\n",
    "            if return_pd.shape[0] > 50000:\n",
    "                print(\"We've scrapped the \" + str(counter) + \"th 50000 row in our data, ouputing a csv...\")\n",
    "                return_pd.to_csv(\"realdata\" + str(counter) + \".csv\")\n",
    "                return_pd = pd.DataFrame([])\n",
    "                print(\"The \" + str(counter) + \"th 50000 row has been successfully output as\" + \"realdata\" + str(counter) + \".csv\")\n",
    "                counter += 1\n",
    "        except:\n",
    "                print(\"Unfortunately, there is an error occured, now outputing the remaining data to a csv...\")\n",
    "                print(course_link)\n",
    "                print(course_lst.index(course_link))\n",
    "                # outputing the result data to a file named realdata.csv\n",
    "                return_pd.to_csv(\"realdata.csv\")\n",
    "    # destroy driver instance\n",
    "    driver.quit()\n",
    "    print(\"Congradualation! The last record has been recorded, now outputing the data to a csv...\")\n",
    "    # outputing the result data to a file named realdata.csv\n",
    "    return_pd.to_csv(\"realdata.csv\")\n",
    "    print('The remaining data has been output to a file named \"realdata.csv\". Goodbye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1334e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_func():\n",
    "    soup, driver = get_raw_html_noquit()\n",
    "    print(\"Getting a list of course IDs for scraping the individual pages...\")\n",
    "    course_lst = get_courseID(soup)\n",
    "    print('This browser window will now keep changing until the last cape record has been recorded...')\n",
    "    return_pd = pd.DataFrame([])\n",
    "    counter = 37 # counting the slice of csv files ouput\n",
    "    start_index = course_lst.index(\"CAPEReport.aspx?sectionid=779432\") + 1\n",
    "    course_link = \"https://cape.ucsd.edu/responses/\" + course_lst[start_index]\n",
    "    # get each individual course page\n",
    "    driver.get(course_link)\n",
    "    for course in range(start_index + 1, len(course_lst)):\n",
    "        # creating soup object\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "        # preventing hitting error with pages with wrong link, smh who made cape page should be jailed!\n",
    "        if soup.find(\"span\", {\"class\": \"errormessage\"}).text != \"\":\n",
    "            print(\"Error message find\")\n",
    "            print(course_link)\n",
    "            course_link = \"https://cape.ucsd.edu/responses/\" + course_lst[course + 1]\n",
    "            driver.get(course_link)\n",
    "            continue\n",
    "        try:\n",
    "            # Getting the actual grade received distribution\n",
    "            df = pd.read_html(driver.page_source)\n",
    "            # Since we know prior to FA12, CAPE uses old format, we would exclude it from this function\n",
    "            if course_lst[course + 1] == \"../scripts/detailedStats.asp?SectionId=749583\":\n",
    "                break\n",
    "            # There is a slight chance that the url may contain A-z\n",
    "            course_link = \"https://cape.ucsd.edu/responses/\" + course_lst[course + 1]#.strip(\"CAPEReport.aspx?sectionid=\")#.strip(\"[A-z]+\")\n",
    "            # get each individual course page\n",
    "            try:\n",
    "                driver.get(course_link)\n",
    "            except:\n",
    "                try:\n",
    "                    driver.get(course_link)\n",
    "                except:\n",
    "                    print(\"Unfortunately, there is an error occured, now outputing the remaining data to a csv...\")\n",
    "                    print(course_link)\n",
    "                    print(course_lst.index(course_link))\n",
    "                    # outputing the result data to a file named realdata.csv\n",
    "                    return_pd.to_csv(\"realdata.csv\")\n",
    "                    print(course_link)\n",
    "                    break\n",
    "            \n",
    "            # return an individual dataframe object for one page\n",
    "            return_pd = pd.concat([return_pd, question_collector_to_pd(soup)])\n",
    "            if len(df) > 2:\n",
    "                df = df[2]\n",
    "                df[\"term\"] = return_pd[\"term\"].iloc[-1]\n",
    "                df[\"course\"] = return_pd[\"course\"].iloc[-1]\n",
    "                df[\"Question\"] = \"Grade received\"\n",
    "                return_pd = pd.concat([return_pd, df])\n",
    "            if return_pd.shape[0] > 50000:\n",
    "                print(\"We've scrapped the \" + str(counter) + \"th 50000 row in our data, ouputing a csv...\")\n",
    "                return_pd.to_csv(\"realdata\" + str(counter) + \".csv\")\n",
    "                return_pd = pd.DataFrame([])\n",
    "                print(\"The \" + str(counter) + \"th 50000 row has been successfully output as \" + \"realdata\" + str(counter) + \".csv\")\n",
    "                print(\"Last Entry is:\")\n",
    "                print(course_link)\n",
    "                counter += 1\n",
    "        except:\n",
    "                print(\"Unfortunately, there is an error occured, now outputing the remaining data to a csv...\")\n",
    "                print(course_link)\n",
    "                print(course)\n",
    "                # outputing the result data to a file named realdata.csv\n",
    "                return_pd.to_csv(\"realdata.csv\")\n",
    "                break\n",
    "    \n",
    "    # destroy driver instance\n",
    "    driver.quit()\n",
    "    print(\"Congradualation! The last record has been recorded, now outputing the data to a csv...\")\n",
    "    print(course_link)\n",
    "    # outputing the result data to a file named realdata.csv\n",
    "    return_pd.to_csv(\"realdata.csv\")\n",
    "    print('The remaining data has been output to a file named \"realdata.csv\". Goodbye!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7707929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lst_tester():\n",
    "    soup, driver = get_raw_html_noquit()\n",
    "    print(\"Getting a list of course IDs for scraping the individual pages...\")\n",
    "    course_lst = get_courseID(soup)\n",
    "    return course_lst"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40ec875e",
   "metadata": {},
   "source": [
    "lst_check = lst_tester()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "118a71a7",
   "metadata": {},
   "source": [
    "lst_check.index(\"CAPEReport.aspx?sectionid=037745\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30efd66a",
   "metadata": {},
   "source": [
    "len(lst_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b15a1d20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current firefox version is 100.0\n",
      "Get LATEST geckodriver version for 100.0 firefox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a browser window...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Driver [/Users/edwint/.wdm/drivers/geckodriver/macos/v0.31.0/geckodriver] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser window open, loading the page...\n",
      "Please enter credentials...\n",
      "Now loading...\n",
      "Please wait patiently as it may take longer than a few minutes to complete due to the large size of the CAPE dataset\n",
      "Finished loading, currently parsing the soup object...\n",
      "Soup parsed.\n",
      "Getting a list of course IDs for scraping the individual pages...\n",
      "This browser window will now keep changing until the last cape record has been recorded...\n",
      "We've scrapped the 37th 50000 row in our data, ouputing a csv...\n",
      "The 37th 50000 row has been successfully output as realdata37.csv\n",
      "Last Entry is:\n",
      "https://cape.ucsd.edu/responses/CAPEReport.aspx?sectionid=772394\n",
      "We've scrapped the 38th 50000 row in our data, ouputing a csv...\n",
      "The 38th 50000 row has been successfully output as realdata38.csv\n",
      "Last Entry is:\n",
      "https://cape.ucsd.edu/responses/CAPEReport.aspx?sectionid=765020\n",
      "We've scrapped the 39th 50000 row in our data, ouputing a csv...\n",
      "The 39th 50000 row has been successfully output as realdata39.csv\n",
      "Last Entry is:\n",
      "https://cape.ucsd.edu/responses/CAPEReport.aspx?sectionid=755824\n",
      "Congradualation! The last record has been recorded, now outputing the data to a csv...\n",
      "https://cape.ucsd.edu/responses/CAPEReport.aspx?sectionid=749898\n",
      "The remaining data has been output to a file named \"realdata.csv\". Goodbye!\n"
     ]
    }
   ],
   "source": [
    "cookies = scrape_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "843eed3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst_check' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/51/64d5lv_53rx716ghj8z3q13c0000gn/T/ipykernel_1176/3633412906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp_lstarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msplit_arr_check\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_lstarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lst_check' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np_lstarr = np.asarray(lst_check)\n",
    "split_arr_check = np.char.split(np_lstarr, sep = \"=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759a4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_arr_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e2d5fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np_lstarr[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a6997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"936287A\".strip(\"[A-z]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d5a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lst_series = pd.Series(np.char.strip(np.char.strip(np_lstarr, \"CAPEReport.aspx?sectionid=\"\n",
    "                                         ),\"/scripts/detailedStats.asp?SectionId=\"))\n",
    "lst_series[lst_series.str.contains(\"E\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f431e93a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
